{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3134abfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e2bada",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b79b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed \n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "train_ratio = 0.8\n",
    "validation_ratio = 0.2\n",
    "batch_size = 64\n",
    "lr = [0.01, 0.001, 0.0001]\n",
    "momentum = [0.9, 0.001, 0.0001]\n",
    "weight_decay = [0.1, 0.01, 0.001, 0.0001]\n",
    "dropout_prob = [0.1, 0.5]\n",
    "training_epochs = 50\n",
    "final_model_epochs = 10\n",
    "number_of_features = 64 * 64 * 3\n",
    "class_labels = ('Airplane', 'Bird', 'Car', 'Cat', 'Deer', 'Dog', 'Horse', 'Monkey', 'Ship', 'Truck')\n",
    "\n",
    "# set seed and GPU settings\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.enabled = use_cuda\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d8f511",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888286c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = (0.43, 0.42, 0.39), (0.27, 0.26, 0.27)\n",
    "\n",
    "plot_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                          transforms.Normalize(mean, std)])\n",
    "\n",
    "train_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                          transforms.Normalize(mean, std),\n",
    "                                          transforms.RandomCrop(64),\n",
    "                                          transforms.RandomHorizontalFlip(p=0.5)])\n",
    "                                          #transforms.RandomRotation(5),\n",
    "                                          #transforms.ColorJitter(brightness=0.01, contrast=0.01, saturation=0.01)])\n",
    "                                          #transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))])\n",
    "                                          #transforms.RandomPerspective(distortion_scale=0.4)])\n",
    "                        \n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean, std),\n",
    "                                         transforms.CenterCrop(64)])\n",
    "\n",
    "plot_dataset = datasets.STL10(root='./', split='train', transform=plot_transform)    \n",
    "train_dataset = datasets.STL10(root='./', split='train', transform=train_transform)\n",
    "val_dataset = datasets.STL10(root='./', split='train', transform=test_transform)\n",
    "test_dataset = datasets.STL10(root='./', split='test', transform=test_transform)\n",
    "\n",
    "targets = train_dataset.labels\n",
    "targets_idx = np.arange(len(targets))\n",
    "train_idx, val_idx = train_test_split(targets_idx, test_size=validation_ratio, random_state=seed,\n",
    "                                        shuffle=True, stratify=targets)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "plot_loader = DataLoader(plot_dataset, shuffle=False, num_workers=0)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                           num_workers=2, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                         num_workers=0, sampler=val_sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f4e6f",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bbc209",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 1, figsize=(15, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "data_iter = iter(plot_loader)\n",
    "for index, c in enumerate(class_labels):\n",
    "    counter = 0\n",
    "    class_images = torch.empty(4, 3, 96, 96)\n",
    "    while counter < 4:\n",
    "        image, label = data_iter.next()\n",
    "        if label == index:\n",
    "            class_images[counter] = image\n",
    "            counter += 1\n",
    "    class_images = class_images / 2 + 0.5  # denormalize\n",
    "    axes[index].imshow(np.transpose(make_grid(class_images), (1, 2, 0)))\n",
    "    axes[index].set_ylabel(str(c), rotation='horizontal', fontsize=20,\n",
    "                           verticalalignment='center', horizontalalignment='right')\n",
    "for ax in axes:\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eab4b57",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df46c49",
   "metadata": {},
   "source": [
    "### General functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9670082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_of_batches = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            num_of_batches += 1\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "    return correct / total, test_loss / num_of_batches\n",
    "\n",
    "def train_model(model, train_loader, training_epochs, lr, momentum, weight_decay):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    train_losses, train_accuracy, val_losses, val_accuracy = ([] for i in range(4))\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            train_correct = 0.\n",
    "            train_total = 0.\n",
    "            num_of_batches = 0\n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "                num_of_batches += 1\n",
    "\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_losses.append(running_loss / num_of_batches)\n",
    "            train_accuracy.append(train_correct / train_total)\n",
    "            val_check = test(val_loader, model)\n",
    "            val_accuracy.append(val_check[0])\n",
    "            val_losses.append(val_check[1])\n",
    "            #print('epoch %d \\tloss: %.3f\\t  acc:%.3f\\t val_acc:%.3f' %\n",
    "              #(epoch + 1, running_loss / num_of_batches, train_correct / train_total, val_check[0]))\n",
    "    results = train_accuracy, train_losses, val_accuracy, val_losses\n",
    "    return model, results, val_accuracy[len(val_accuracy) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fb93d7",
   "metadata": {},
   "source": [
    "### Hyper-parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe28c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_parameters_tuning(best_params_path, model):\n",
    "    with open(best_params_path, \"r\") as file:\n",
    "            best_acc = file.readline()\n",
    "    for hyper_parameters_set in itertools.product(lr, momentum, weight_decay):\n",
    "        best_model, results, acc_iterate = train_model(model, train_loader,\n",
    "                                      training_epochs, hyper_parameters_set[0], hyper_parameters_set[1],\n",
    "                                                       hyper_parameters_set[2])\n",
    "        \n",
    "        print(\"lr: \" + str(hyper_parameters_set[0]) + \", momentum: \" + str(hyper_parameters_set[1])\n",
    "          + \", weight_decay: \" + str(hyper_parameters_set[2]) + \", accuracy: \"+ str(acc_iterate))\n",
    "        if acc_iterate > float(best_acc):\n",
    "            best_acc = acc_iterate\n",
    "            print(\"new best accuracy: \"+ str(best_acc))\n",
    "            f_w = open(best_params_path, \"w\")\n",
    "            f_w.write(str(best_acc)+'\\n')\n",
    "            f_w.write(str(hyper_parameters_set[0]) + '\\n')\n",
    "            f_w.write(str(hyper_parameters_set[1]) + '\\n')\n",
    "            f_w.write(str(hyper_parameters_set[2]) + '\\n')\n",
    "            f_w.close()\n",
    "        \n",
    "        #lines = open(best_params_path, 'r').readlines()\n",
    "        #train_accuracy, train_losses, val_accuracy, val_losses = results\n",
    "        #steps = np.arange(training_epochs)\n",
    "        #fig, ax1 = plt.subplots()\n",
    "        #ax1.set_xlabel('epochs')\n",
    "        #ax1.set_ylabel('loss')\n",
    "        # ax1.set_title('test loss: %.3f, test accuracy: %.3f' % (test_loss, test_acc))\n",
    "        #ax1.plot(steps, train_losses, label=\"train loss\", color='red')\n",
    "        #ax1.plot(steps, val_losses, label=\"val loss\", color='green')\n",
    "\n",
    "        #ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "        #ax2.set_ylabel('accuracy')  # we already handled the x-label with ax1\n",
    "        #ax2.plot(steps, train_accuracy, label=\"train acc\", color='black')\n",
    "        #ax2.plot(steps, val_accuracy, label=\"val acc\", color='blue')\n",
    "\n",
    "        #fig.legend(loc='center right', bbox_to_anchor=(0.8, 0.6))\n",
    "        #fig.suptitle('Epochs={}, LR={}, momentum={}, reg={}'.format(training_epochs, lines[1], lines[2], lines[3]))\n",
    "        #fig.tight_layout()\n",
    "        #\n",
    "        plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a6ee34",
   "metadata": {},
   "source": [
    "### Train best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74a0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model(best_params_path, model, figure_name):\n",
    "    \n",
    "    f_r = open(best_params_path, \"r\")\n",
    "    Lines = f_r.read().splitlines()\n",
    "    f_r.close()\n",
    "    best_params = []\n",
    "    for line in Lines:\n",
    "        best_params.append(line)\n",
    "    \n",
    "    best_model, results, acc_iterate = train_model(model, train_loader, final_model_epochs, float(best_params[1]),\n",
    "                                                   float(best_params[2]), float(best_params[3]))\n",
    "    \n",
    "    train_accuracy, train_losses, val_accuracy, val_losses = results\n",
    "    steps = np.arange(final_model_epochs)\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.set_xlabel('epochs')\n",
    "    ax1.set_ylabel('loss')\n",
    "    # ax1.set_title('test loss: %.3f, test accuracy: %.3f' % (test_loss, test_acc))\n",
    "    ax1.plot(steps, train_losses, label=\"train loss\", color='red')\n",
    "    ax1.plot(steps, val_losses, label=\"val loss\", color='green')\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    ax2.set_ylabel('accuracy')  # we already handled the x-label with ax1\n",
    "    ax2.plot(steps, train_accuracy, label=\"train acc\", color='black')\n",
    "    ax2.plot(steps, val_accuracy, label=\"val acc\", color='blue')\n",
    "\n",
    "    fig.legend(loc='center right', bbox_to_anchor=(0.8, 0.6))\n",
    "    fig.suptitle('Epochs={}, LR={}, momentum={}, reg={}'.format(final_model_epochs, best_params[1], best_params[2], best_params[3]))\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(figure_name)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65ff540",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee8717b",
   "metadata": {},
   "source": [
    "### Definintion of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7cf782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, number_of_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.flat = nn.Flatten(start_dim=1)\n",
    "        self.linear = nn.Linear(number_of_features, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flat(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2afc22",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ad4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters_tuning(\"./lr_best_params.txt\", LogisticRegression(number_of_features).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43033d7f",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f31bc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_model(\"./lr_best_params.txt\", LogisticRegression(number_of_features).to(device), \"lr_plot.png\")\n",
    "print(\"~~~~~~~~~~~~~~~Logistic Regression Done~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8647b0be",
   "metadata": {},
   "source": [
    "## Fully Connected NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93df8c1b",
   "metadata": {},
   "source": [
    "### Definintion of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1187e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc3_nn(nn.Module):\n",
    "    def __init__(self, number_of_features, 200, 100, dropout_prob):\n",
    "        super(fc3_nn, self).__init__()\n",
    "        self.flat = nn.Flatten(start_dim=1)\n",
    "        # first layer\n",
    "        self.first_layer = nn.Linear(number_of_features, 200)\n",
    "        self.first_layer_norm = nn.BatchNorm1d(200)\n",
    "        self.first_layer_dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        # second layer\n",
    "        self.second_layer = nn.Linear(200, 200)\n",
    "        self.second_layer_norm = nn.BatchNorm1d(200)\n",
    "        self.second_layer_dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        # third layer\n",
    "        self.third_layer = nn.Linear(200, 100)\n",
    "        self.third_layer_norm = nn.BatchNorm1d(100)\n",
    "        self.third_layer_dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.last_layer = nn.Linear(100, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flat(x)\n",
    "        x = self.first_layer(x)\n",
    "        x = self.first_layer_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.first_layer_dropout(x)\n",
    "        \n",
    "        x = self.second_layer(x)\n",
    "        x = self.second_layer_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.second_layer_dropout(x)\n",
    "        \n",
    "        x = self.third_layer(x)\n",
    "        x = self.third_layer_norm(x)\n",
    "        x = F.relu(x)        \n",
    "        x = self.third_layer_dropout(x)\n",
    "        x = self.last_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882caa6a",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a208bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./cnn_best_params.txt\", \"r\") as file:\n",
    "    max_acc = file.readline()\n",
    "\n",
    "for prob in dropout_prob:\n",
    "    print(\"Dropout probability: \" + str(prob))\n",
    "    hyper_parameters_tuning(\"./fc3_nn_best_params.txt\", fc3_nn(number_of_features, 200, 100, prob).to(device))\n",
    "    lines = open(\"./cnn_best_params.txt\", 'r').readlines()\n",
    "    next_max_acc = lines[0]\n",
    "    \n",
    "    # check if need to update the new dropout probability\n",
    "    if max_acc < next_max_acc:\n",
    "        max_acc = next_max_acc\n",
    "        lines[-1] = str(prob)\n",
    "        open(\"./cnn_best_params.txt\", 'w').writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c926ff3e",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b9cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(\"./fc3_nn_best_params.txt\", 'r').readlines()\n",
    "train_final_model(\"./fc3_nn_best_params.txt\", fc3_nn(number_of_features, 200, 100, float(lines[-1])).to(device), \"fc3_nn_plot.png\")\n",
    "print(\"~~~~~~~~~~~~~~~Fully-Connected NN Done~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05447972",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2b8afb",
   "metadata": {},
   "source": [
    "### Definintion of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a773438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self, number_of_features, dropout_prob):\n",
    "        super(cnn, self).__init__()\n",
    "        self.first_conv = nn.Conv2d(3, 6, 3)\n",
    "        self.first_conv_norm = nn.BatchNorm2d(6)\n",
    "        self.first_maxpool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.second_conv = nn.Conv2d(6, 15, 2)\n",
    "        self.second_conv_norm = nn.BatchNorm2d(15)\n",
    "        self.second_maxpool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.third_conv = nn.Conv2d(15, 30, 2)\n",
    "        self.third_conv_norm = nn.BatchNorm2d(30)\n",
    "        self.third_maxpool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.first_linear = nn.Linear(30 * 7 * 7, 500)\n",
    "        self.first_dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        self.second_linear = nn.Linear(500, 100)\n",
    "        self.second_dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.last_layer = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_conv(x)\n",
    "        x = self.first_conv_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.first_maxpool(x)\n",
    "        \n",
    "        x = self.second_conv(x)\n",
    "        x = self.second_conv_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.second_maxpool(x)\n",
    "        \n",
    "        x = self.third_conv(x)\n",
    "        x = self.third_conv_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.third_maxpool(x)\n",
    "        \n",
    "        x = x.view(-1, 30 * 7 * 7)\n",
    "\n",
    "        x = self.first_linear(x)\n",
    "        x = self.first_dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.second_linear(x)\n",
    "        x = self.second_dropout(x)\n",
    "        \n",
    "        x = self.last_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07b78e1",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaef3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./cnn_best_params.txt\", \"r\") as file:\n",
    "    max_acc = file.readline()\n",
    "\n",
    "for prob in dropout_prob:\n",
    "    print(\"Dropout probability: \" + str(prob))\n",
    "    hyper_parameters_tuning(\"./cnn_best_params.txt\", cnn(number_of_features, prob).to(device))\n",
    "    lines = open(\"./cnn_best_params.txt\", 'r').readlines()\n",
    "    next_max_acc = lines[0]\n",
    "    \n",
    "    # check if need to update the new dropout probability\n",
    "    if max_acc < next_max_acc:\n",
    "        max_acc = next_max_acc\n",
    "        lines[-1] = str(prob)\n",
    "        open(\"./cnn_best_params.txt\", 'w').writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0bbe63",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb52f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(\"./cnn_best_params.txt\", 'r').readlines()\n",
    "train_final_model(\"./cnn_best_params.txt\", cnn(number_of_features, float(lines[-1])).to(device), \"cnn_plot.png\")\n",
    "print(\"~~~~~~~~~~~~~~~CNN Done~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7458df11",
   "metadata": {},
   "source": [
    "## Fixed pre-trained MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77227254",
   "metadata": {},
   "source": [
    "### Definintion of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e6b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fixed_MobileNetV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Fixed_MobileNetV2, self).__init__()\n",
    "        self.feature_extractor = models.mobilenet.mobilenet_v2(pretrained=True)\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.feature_extractor.classifier[1] = nn.Linear(self.feature_extractor.classifier[1].in_features,out_features=500)\n",
    "        \n",
    "        self.d1 = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(500, 100)\n",
    "        self.d2 = nn.Dropout(p=0.2)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "        self.d3 = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = F.relu(x)        \n",
    "        x = self.d1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.d2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.d3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff1bcb8",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc23c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters_tuning(\"./f_mobilenet_best_params.txt\", Fixed_MobileNetV2().to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f33a0f3",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4204e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_model(\"./f_mobilenet_best_params.txt\", Fixed_MobileNetV2().to(device),\n",
    "                  \"f_mobilenet_plot.png\")\n",
    "print(\"~~~~~~~~~~~~~~~Fixed MobileNetV2 Done~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef06e00",
   "metadata": {},
   "source": [
    "## Learned pre-trained MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868e58d1",
   "metadata": {},
   "source": [
    "### Definintion of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ee25d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learned_MobileNetV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Learned_MobileNetV2, self).__init__()\n",
    "        self.feature_extractor = models.mobilenet.mobilenet_v2(pretrained=True)\n",
    "        self.feature_extractor.classifier[1] = nn.Linear(self.feature_extractor.classifier[1].in_features,out_features=500)\n",
    "        \n",
    "        self.d1 = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(500, 100)\n",
    "        self.d2 = nn.Dropout(p=0.2)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "        self.d3 = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = F.relu(x)        \n",
    "        x = self.d1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.d2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.d3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba835f8",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e2be44",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters_tuning(\"./f_mobilenet_best_params.txt\", Fixed_MobileNetV2().to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61840ded",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37b7cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_model(\"./l_mobilenet_best_params.txt\", Learned_MobileNetV2().to(device),\n",
    "                  \"l_mobilenet_plot.png\")\n",
    "print(\"~~~~~~~~~~~~~~~Learned MobileNetV2 Done~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa1a434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

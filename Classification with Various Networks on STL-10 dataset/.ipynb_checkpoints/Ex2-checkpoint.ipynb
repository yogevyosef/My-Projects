{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b643f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a07c42c",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c27625",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./train.csv\", header=0)\n",
    "X_test = pd.read_csv(\"./test.csv\", header=0)\n",
    "epsilon = 10e-10\n",
    "probability_to_delete_neural = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b324ac7",
   "metadata": {},
   "source": [
    "## General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ba84d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(X, Y):\n",
    "    grid = np.zeros((10, 4, 28, 28))\n",
    "    labelToNameDict = {\n",
    "      0: \"Top          \",\n",
    "      1: \"Trouser              \",\n",
    "      2: \"Pullover               \",\n",
    "      3: \"Dress          \",\n",
    "      4: \"Coat          \",\n",
    "      5: \"Sandal              \",\n",
    "      6: \"Shirt          \",\n",
    "      7: \"Sneaker               \",\n",
    "      8: \"Bag          \",\n",
    "      9: \"Ankle boot                   \",\n",
    "    }\n",
    "    \n",
    "    for i in range(10):\n",
    "        numToPrint = 0\n",
    "        index = 0\n",
    "        while(numToPrint != 4):\n",
    "            if Y[index] == i:\n",
    "                grid[i][numToPrint] = X[index].reshape(28,28)\n",
    "                numToPrint = numToPrint +1            \n",
    "            index += 1\n",
    "    plt.figure(figsize=(10, 4))  # specifying the overall grid size\n",
    "\n",
    "    for i in range(10):\n",
    "        for j in range(4):\n",
    "            if j == 1:\n",
    "                plt.ylabel(labelToNameDict[i], rotation=\"horizontal\")\n",
    "            plt.subplot(10, 4, i*4 + j + 1)\n",
    "            plt.imshow(grid[i][j])\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# X is (N x M) = (56,000 x 784)\n",
    "# Y is (N x 1) = (56,000 x 1)\n",
    "def shuffle_split_data(X, Y):\n",
    "    split = np.random.rand(X.shape[0]) < 0.7\n",
    "    X_Train = X[split]\n",
    "    Y_Train = Y[split]\n",
    "    X_Test =  X[~split]\n",
    "    Y_Test = Y[~split]\n",
    "    \n",
    "    return X_Train, Y_Train, X_Test, Y_Test\n",
    "\n",
    "\n",
    "# z is (N x C) = (N x 10)\n",
    "# returned value is (N x C)\n",
    "def softmax(z):\n",
    "    # Z = - X @ W\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis]\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis]\n",
    "    return e_x / div\n",
    "\n",
    "\n",
    "# X is (N x M) = (56,000 x 784)\n",
    "def min_max_norm(X, min, max):\n",
    "    return (X - min)/(max - min + epsilon)\n",
    "\n",
    "\n",
    "# Y is (N x 1)\n",
    "# returned value is (N x C)\n",
    "def makeOnehotMatrix(Y):\n",
    "    onehotMat = np.zeros((Y.shape[0], 10))\n",
    "    for i in range(onehotMat.shape[0]):\n",
    "        index = Y[i]\n",
    "        onehotMat[i][index] = 1\n",
    "    return onehotMat\n",
    "\n",
    "\n",
    "# Y_softmax is (N x C)\n",
    "# Y_onehot is (N x C)\n",
    "# returned value is (scalar)\n",
    "def cross_entropy(Y_softmax, Y_onehot):\n",
    "    # input NxC matrices\n",
    "    # output scalar\n",
    "    N = Y_softmax.shape[0]\n",
    "    loss = np.sum(np.log(Y_softmax + epsilon) * Y_onehot)\n",
    "    return -loss / N\n",
    "\n",
    "\n",
    "def plot_loss(train_losses, train_accuracy, val_losses, val_accuracy, epochs, batch_size,\n",
    "              learning_rate, mu, layer_size=0):\n",
    "    steps = np.arange(epochs)\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.set_xlabel(\"epochs\")\n",
    "    ax1.set_ylabel(\"loss\")\n",
    "    ax1.plot(steps, train_losses, label=\"train loss\", color='red')\n",
    "    ax1.plot(steps, val_losses, label=\"val loss\", color='green')\n",
    "    \n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    ax2.set_ylabel(\"accuracy\") # x label is taken from ax1\n",
    "    ax2.plot(steps, train_accuracy, label=\"train acc\", color='brown')\n",
    "    ax2.plot(steps, val_accuracy, label=\"val acc\", color='blue')\n",
    "    \n",
    "    fig.legend(loc='center right', bbox_to_anchor=(0.8, 0.6))\n",
    "    if layer_size == 0:\n",
    "        fig.suptitle(\n",
    "            'LR: learning rate={}, batch size={}, mu={}'.format(learning_rate, batch_size, mu))\n",
    "    else:\n",
    "        fig.suptitle('NN: learning rate={}, batch size={}, mu={}, layer size={}'.format(learning_rate, batch_size,\n",
    "                                                                                                mu, layer_size))\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75503737",
   "metadata": {},
   "source": [
    "## Organize and normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5cbd556",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(df)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "Y = data[:, 0]\n",
    "X = data[:, 1:]\n",
    "X_train, Y_train, X_val, Y_val = shuffle_split_data(X, Y)\n",
    "\n",
    "# create vector of min/max\n",
    "min = np.min(X_train, axis = 0)\n",
    "max = np.max(X_train, axis = 0)\n",
    "\n",
    "X_train = min_max_norm(X_train, min, max)\n",
    "X_val = min_max_norm(X_val, min, max)\n",
    "X_test = min_max_norm(X_test, min, max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91434e6",
   "metadata": {},
   "source": [
    "## Logistic Regression functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56985bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is (N x M)\n",
    "# W is (M x C)\n",
    "# returned value is (N x 1)\n",
    "def predict_lr(W, X_test):\n",
    "    P = softmax(- X_test @ W)\n",
    "    return np.argmax(P, axis=1)\n",
    "\n",
    "\n",
    "# X is (N x M)\n",
    "# W is (M x C)\n",
    "# Y_onehot is (N x C)\n",
    "# returned value is (M x C)\n",
    "def gradient(X, W, Y_onehot, mu):\n",
    "    # Y is onehot encoded\n",
    "    N = X.shape[0]\n",
    "    Z = softmax(- X @ W)\n",
    "    return (1/N * X.T @ (Y_onehot - Z) + 2 * mu * W)\n",
    "\n",
    "\n",
    "def gradient_descent_lr(X_train, Y_train, X_val, Y_val, batch_size = 256, learning_rate=0.01, mu=0.0001, max_epoch=300):\n",
    "    Y_train_onehot = makeOnehotMatrix(Y_train)\n",
    "    Y_val_onehot = makeOnehotMatrix(Y_val)\n",
    "    num_of_batches = int(X_train.shape[0] / batch_size)\n",
    "    X_train_batches = np.array_split(X_train, num_of_batches)\n",
    "    Y_train_batches = np.array_split(Y_train, num_of_batches)\n",
    "    Y_onehot_batches = np.array_split(Y_train_onehot, num_of_batches)\n",
    "    W = np.random.rand(X_train.shape[1], Y_train_onehot.shape[1]) \n",
    "    epoch = 0\n",
    "    loss_train_lst = []\n",
    "    acc_train_lst = []\n",
    "    loss_val_lst = []\n",
    "    acc_val_lst = []\n",
    "    \n",
    "    while epoch < max_epoch:\n",
    "        epoch += 1\n",
    "        for i in range (num_of_batches):\n",
    "            W -= learning_rate * gradient(X_train_batches[i], W, Y_onehot_batches[i], mu)\n",
    "        \n",
    "        #calculate accuracy of train data per epoch\n",
    "        score_train = predict_lr(W, X_train)\n",
    "        grades_train = score_train == Y_train.T\n",
    "        acc_train = sum(grades_train.T)/len(Y_train)\n",
    "\n",
    "        #calculate accuracy of validation data per epoch\n",
    "        score_val = predict_lr(W, X_val)\n",
    "        grades_val = score_val == Y_val.T\n",
    "        acc_val = sum(grades_val.T)/len(Y_val)\n",
    "        \n",
    "        acc_val_lst.append(acc_val)\n",
    "        acc_train_lst.append(acc_train)\n",
    "         \n",
    "        #calculate loss of train&validation data per epoch\n",
    "        regularization = mu * np.sum(np.power(W, 2))\n",
    "        loss_train_lst.append(cross_entropy(softmax(- X_train @ W), Y_train_onehot) + regularization)\n",
    "        loss_val_lst.append(cross_entropy(softmax(- X_val @ W), Y_val_onehot) + regularization)\n",
    "        \n",
    "        \n",
    "    plot_df = pd.DataFrame({\n",
    "        'loss_train': loss_train_lst,\n",
    "        'accuracy_train': acc_train_lst,\n",
    "        'loss_val': loss_val_lst,\n",
    "        'accuracy_val': acc_val_lst})\n",
    "    \n",
    "    return plot_df, W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79963bfd",
   "metadata": {},
   "source": [
    "## NN functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f71b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid_dx(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "\n",
    "def drelu_dx(x):\n",
    "    y = np.zeros_like(x)\n",
    "    y[x > 0] = 1\n",
    "    return y\n",
    "\n",
    "\n",
    "# X is (N x M)\n",
    "# returned value is (N x 1)\n",
    "def predict_nn(X_val, W1, b1, W2, b2, predict_type = \"prediction\", activation_function = \"sigmoid\"):\n",
    "    Z1 = X_val @ W1 + b1\n",
    "    if activation_function == \"sigmoid\":\n",
    "        h = sigmoid(Z1)\n",
    "    else:\n",
    "        h = relu(Z1)\n",
    "    \n",
    "    # fix the weights of h in the prediction\n",
    "    if predict_type == \"prediction\":\n",
    "        probability_to_delete_neural * h\n",
    "        \n",
    "    Z2 = h @ W2 + b2\n",
    "    Y_softmax = softmax(Z2)\n",
    "    return np.argmax(Y_softmax, axis=1)\n",
    "\n",
    "\n",
    "# X_train is (N1 x M)\n",
    "# Y_train is (N1 x 1)\n",
    "# N1 + N2 = N = 56,000\n",
    "# X_val is (N2 x M)\n",
    "# Y_val is (N2 x 1)\n",
    "def gradient_descent_nn(X_train, Y_train, X_val, Y_val, batch_size = 512, learning_rate=0.01, mu=0.0001, layer_size = 400,\n",
    "                        activation_function = \"sigmoid\", epochs=50):\n",
    "    Y_train_onehot = makeOnehotMatrix(Y_train)\n",
    "    Y_val_onehot = makeOnehotMatrix(Y_val) \n",
    "    W1 = np.random.randn(X_train.shape[1], layer_size)\n",
    "    b1 = np.zeros((1, layer_size))\n",
    "    W2 = np.random.randn(layer_size, 10)\n",
    "    b2 = np.zeros((1, 10))\n",
    "    \n",
    "    num_of_batches = int(X_train.shape[0] / batch_size)\n",
    "    X_train_batches = np.array_split(X_train, num_of_batches)\n",
    "    Y_onehot_batches = np.array_split(Y_train_onehot, num_of_batches)\n",
    "    loss_train_lst = []\n",
    "    acc_train_lst = []\n",
    "    loss_val_lst = []\n",
    "    acc_val_lst = []\n",
    "    \n",
    "    for j in range(epochs):\n",
    "        for i in range(num_of_batches):\n",
    "            # forward\n",
    "            Z1 = X_train_batches[i] @ W1 + b1\n",
    "            if activation_function == \"sigmoid\":\n",
    "                h = sigmoid(Z1)\n",
    "            else:\n",
    "                h = relu(Z1)\n",
    "                \n",
    "            dropout_mat = (np.random.rand(h.shape[0], h.shape[1]) > probability_to_delete_neural).astype(int)\n",
    "            h = h * dropout_mat\n",
    "            \n",
    "            Z2 = h @ W2 + b2\n",
    "            \n",
    "            Y_softmax = softmax(Z2)\n",
    "            # backward\n",
    "            N = X_train_batches[i].shape[0]\n",
    "            \n",
    "            grad_z2 = Y_softmax - Y_onehot_batches[i]\n",
    "            grad_b2 = (1 / N) * np.sum(grad_z2, axis=0)\n",
    "            grad_W2 = (1 / N) * h.T @ grad_z2\n",
    "            grad_h = (1 / N) * grad_z2 @ W2.T\n",
    "            \n",
    "            grad_h = grad_h * dropout_mat\n",
    "            \n",
    "            if activation_function == \"sigmoid\":\n",
    "                grad_z1 = grad_h * dsigmoid_dx(Z1)\n",
    "            else:\n",
    "                grad_z1 = grad_h * drelu_dx(Z1) \n",
    "\n",
    "            grad_b1 = (1 / N) * np.sum(grad_z1, axis=0)\n",
    "            grad_W1 = (1 / N) * X_train_batches[i].T @ grad_z1\n",
    "\n",
    "            regularization1 = mu * W1\n",
    "            regularization2 = mu * W2\n",
    "            \n",
    "            #update\n",
    "            W1 -= learning_rate * (regularization1 + grad_W1)\n",
    "            W2 -= learning_rate * (regularization2 + grad_W2)\n",
    "            b1 -= learning_rate * grad_b1\n",
    "            b2 -= learning_rate * grad_b2\n",
    "            \n",
    "        Z1 = X_train @ W1 + b1\n",
    "        if activation_function == \"sigmoid\":\n",
    "            h = sigmoid(Z1)\n",
    "        else:\n",
    "            h = relu(Z1) \n",
    "        Z2 = h @ W2 + b2\n",
    "        \n",
    "        regularization = mu * (np.sum(np.power(W1, 2)) + np.sum(np.power(W2, 2)))\n",
    "\n",
    "        loss_train_lst.append(cross_entropy(softmax(Z2), Y_train_onehot) + regularization)\n",
    "        \n",
    "        Z1 = X_val @ W1 + b1\n",
    "        if activation_function == \"sigmoid\":\n",
    "            h = sigmoid(Z1)\n",
    "        else:\n",
    "            h = relu(Z1) \n",
    "        Z2 = h @ W2 + b2\n",
    "        \n",
    "        loss_val_lst.append(cross_entropy(softmax(Z2), Y_val_onehot) + regularization)\n",
    "                \n",
    "        #calculate accuracy of train data per epoch\n",
    "        score_train = predict_nn(X_train, W1, b1, W2, b2, \"train\", activation_function)\n",
    "        grades_train = score_train == Y_train.T\n",
    "        acc_train = sum(grades_train.T)/len(Y_train)\n",
    "        acc_train_lst.append(acc_train)\n",
    "        \n",
    "        #calculate accuracy of validation data per epoch\n",
    "        score_val = predict_nn(X_val, W1, b1, W2, b2, \"train\", activation_function)\n",
    "        grades_val = score_val == Y_val.T\n",
    "        acc_val = sum(grades_val.T)/len(Y_val)\n",
    "        acc_val_lst.append(acc_val)\n",
    "\n",
    "\n",
    "    plot_df = pd.DataFrame({\n",
    "        'loss_train': loss_train_lst,\n",
    "        'accuracy_train': acc_train_lst,\n",
    "        'loss_val': loss_val_lst,\n",
    "        'accuracy_val': acc_val_lst})\n",
    "    \n",
    "    return plot_df, W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe84d64f",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd6bf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_images(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5028183",
   "metadata": {},
   "source": [
    "## Part 2 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d566124",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "batches = [128, 256, 512]\n",
    "learning_rates = [1, 0.1, 0.01, 0.001]\n",
    "mu = [0.01, 0.001, 0.0001]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e6743e",
   "metadata": {},
   "source": [
    "#### Hyper-parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eb4427",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_lr_mu in itertools.product(batches, learning_rates, mu):\n",
    "    df, W = gradient_descent_lr(X_train, Y_train, X_val, Y_val, int(batch_lr_mu[0]),\n",
    "                                float(batch_lr_mu[1]), float(batch_lr_mu[2]))\n",
    "    score = predict_lr(W, X_val)\n",
    "    grades = score == Y_val.T\n",
    "    acc_iterate = sum(grades.T)/len(Y_val)\n",
    "\n",
    "    print(\"batch: \"+ str(batch_lr_mu[0]) + \", learning rate: \" + str(batch_lr_mu[1]) + \", mu: \" + str(batch_lr_mu[2]))\n",
    "    print(acc_iterate)\n",
    "    if acc_iterate > best_acc:\n",
    "        best_acc = acc_iterate\n",
    "        print(\"new best accuracy: \"+ str(best_acc))\n",
    "        f_w = open(\"./lr_best_params.txt\", \"w\")\n",
    "        f_w.write(str(best_acc)+'\\n')\n",
    "        f_w.write(str(batch_lr_mu[0]) + '\\n')\n",
    "        f_w.write(str(batch_lr_mu[1]) + '\\n')\n",
    "        f_w.write(str(batch_lr_mu[2]) + '\\n')\n",
    "        f_w.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cab368",
   "metadata": {},
   "source": [
    "#### Read the best parameters and train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be0b070",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_r = open(\"./lr_best_params.txt\", \"r\")\n",
    "Lines = f_r.read().splitlines()\n",
    "f_r.close()\n",
    "lr_best_params = []\n",
    "for line in Lines:\n",
    "    lr_best_params.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c4ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df, W = gradient_descent_lr(X_train, Y_train, X_val, Y_val, int(lr_best_params[1]),\n",
    "                                 float(lr_best_params[2]), float(lr_best_params[3]))\n",
    "\n",
    "plot_loss(plot_df['loss_train'], plot_df['accuracy_train'], plot_df['loss_val'], plot_df['accuracy_val'],\n",
    "              300, int(lr_best_params[1]), float(lr_best_params[2]), float(lr_best_params[3]))\n",
    "\n",
    "best_score = predict_lr(W, X_test)\n",
    "file_name = 'lr_pred.csv'\n",
    "np.savetxt(file_name, best_score, fmt='%i')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061face1",
   "metadata": {},
   "source": [
    "#### Ploting loss&accuracy of batch/learning rate/mu in different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcb62c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in batches:\n",
    "    plot_df, W = gradient_descent_lr(X_train, Y_train, X_val, Y_val, batch,\n",
    "                                     float(lr_best_params[2]), float(lr_best_params[3]), 300)\n",
    "    \n",
    "    plot_loss(plot_df['loss_train'], plot_df['accuracy_train'], plot_df['loss_val'], plot_df['accuracy_val'],\n",
    "              300, batch, float(lr_best_params[2]), float(lr_best_params[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473b58c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in learning_rates:\n",
    "    plot_df, W = gradient_descent_lr(X_train, Y_train, X_val, Y_val, int(lr_best_params[1]),\n",
    "                                     lr, float(lr_best_params[3]), 300)\n",
    "    \n",
    "    plot_loss(plot_df['loss_train'], plot_df['accuracy_train'], plot_df['loss_val'], plot_df['accuracy_val'],\n",
    "              300, int(lr_best_params[1]), lr, float(lr_best_params[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a0ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in mu:\n",
    "    plot_df, W = gradient_descent_lr(X_train, Y_train, X_val, Y_val, int(lr_best_params[1]),\n",
    "                                     float(lr_best_params[2]), m, 300)\n",
    "    \n",
    "    plot_loss(plot_df['loss_train'], plot_df['accuracy_train'], plot_df['loss_val'], plot_df['accuracy_val'],\n",
    "              300, int(lr_best_params[1]), float(lr_best_params[2]), m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825bb9d",
   "metadata": {},
   "source": [
    "## Part 3 - NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3621d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_r = open(\"./nn_best_params.txt\", \"r\")\n",
    "Lines = f_r.read().splitlines()\n",
    "f_r.close()\n",
    "nn_best_params = []\n",
    "for line in Lines:\n",
    "    nn_best_params.append(line)\n",
    "\n",
    "best_acc = float(nn_best_params[0])\n",
    "batches = [256, 512]\n",
    "learning_rates = [1, 0.1, 0.01]\n",
    "mu = [0.001, 0.0001] \n",
    "layer_size = [100, 500] \n",
    "activation_functions = [\"relu\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e936d8",
   "metadata": {},
   "source": [
    "#### Hyper-parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964c1d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_lr_mu_ls_act in itertools.product(batches, learning_rates, mu, layer_size, activation_functions):\n",
    "    plot_df, W1, b1, W2, b2 = gradient_descent_nn(X_train, Y_train, X_val, Y_val, int(batch_lr_mu_ls_act[0]),\n",
    "            float(batch_lr_mu_ls_act[1]), float(batch_lr_mu_ls_act[2]), int(batch_lr_mu_ls_act[3]), batch_lr_mu_ls_act[4])\n",
    "\n",
    "    score = predict_nn(X_val, W1, b1, W2, b2, \"prediction\", batch_lr_mu_ls_act[4])\n",
    "    grades = score == Y_val.T\n",
    "    acc_iterate = sum(grades.T)/len(Y_val)\n",
    "    \n",
    "    print(\"batch: \"+ str(batch_lr_mu_ls_act[0]) + \", learning rate: \" + str(batch_lr_mu_ls_act[1]) +\n",
    "          \", mu: \" + str(batch_lr_mu_ls_act[2]) + \", layer size: \" + str(batch_lr_mu_ls_act[3])\n",
    "          + \", activation function: \" + batch_lr_mu_ls_act[4])\n",
    "    print(acc_iterate)\n",
    "    if acc_iterate > best_acc:\n",
    "        best_acc = acc_iterate\n",
    "        print(\"new best accuracy: \"+ str(best_acc))\n",
    "        f_w = open(\"./nn_best_params.txt\", \"w\")\n",
    "        f_w.write(str(best_acc)+'\\n')\n",
    "        f_w.write(str(batch_lr_mu_ls_act[0]) + '\\n')\n",
    "        f_w.write(str(batch_lr_mu_ls_act[1]) + '\\n')\n",
    "        f_w.write(str(batch_lr_mu_ls_act[2]) + '\\n')\n",
    "        f_w.write(str(batch_lr_mu_ls_act[3]) + '\\n')\n",
    "        f_w.write(str(batch_lr_mu_ls_act[4]) + '\\n')\n",
    "        f_w.close()\n",
    "        \n",
    "    plot_loss(plot_df['loss_train'], plot_df['accuracy_train'], plot_df['loss_val'], plot_df['accuracy_val'],\n",
    "              50, int(batch_lr_mu_ls_act[0]), float(batch_lr_mu_ls_act[1]), float(batch_lr_mu_ls_act[2]),\n",
    "                  int(batch_lr_mu_ls_act[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18b6a55",
   "metadata": {},
   "source": [
    "#### Read the best parameters and train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "794206d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_r = open(\"./nn_best_params.txt\", \"r\")\n",
    "Lines = f_r.read().splitlines()\n",
    "f_r.close()\n",
    "nn_best_params = []\n",
    "for line in Lines:\n",
    "    nn_best_params.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8071bbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df, W1, b1, W2, b2 = gradient_descent_nn(X_train, Y_train, X_val, Y_val, int(nn_best_params[1]),\n",
    "                            float(nn_best_params[2]), float(nn_best_params[3]), int(nn_best_params[4]), nn_best_params[5], 100)\n",
    "\n",
    "plot_loss(plot_df['loss_train'], plot_df['accuracy_train'], plot_df['loss_val'], plot_df['accuracy_val'],\n",
    "              100, int(nn_best_params[1]), float(nn_best_params[2]), float(nn_best_params[3]), nn_best_params[4])\n",
    "\n",
    "best_score = predict_nn(X_test, W1, b1, W2, b2, \"prediction\", nn_best_params[4])\n",
    "file_name = 'NN_pred.csv'\n",
    "np.savetxt(file_name, best_score, fmt='%i')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04be8991",
   "metadata": {},
   "source": [
    "#### Ploting loss&accuracy of batch/learning rate/mu in different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d94b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_for_plot = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc6936",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in batches:\n",
    "    plot_df, W1, b1, W2, b2 = gradient_descent_nn(X_train, Y_train, X_val, Y_val, batch,\n",
    "                            float(nn_best_params[2]), float(nn_best_params[3]), int(nn_best_params[4]), nn_best_params[5])\n",
    "\n",
    "    plot_loss(plot_df['loss_train'], plot_df['accuracy_train'], plot_df['loss_val'], plot_df['accuracy_val'],\n",
    "              50, batch, float(0.001), float(nn_best_params[3]), nn_best_params[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd4eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in learning_rates:\n",
    "    plot_df, W1, b1, W2, b2 = gradient_descent_nn(X_train, Y_train, X_val, Y_val, int(nn_best_params[1]),\n",
    "                                lr, float(nn_best_params[3]), int(nn_best_params[4]), nn_best_params[5])\n",
    "\n",
    "    plot_loss(plot_df['loss_train'], plot_df['accuracy_train'], plot_df['loss_val'], plot_df['accuracy_val'],\n",
    "                  50, int(nn_best_params[1]), lr, float(nn_best_params[3]), nn_best_params[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de3bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in mu:\n",
    "    plot_df, W1, b1, W2, b2 = gradient_descent_nn(X_train, Y_train, X_val, Y_val, int(nn_best_params[1]),\n",
    "                                float(nn_best_params[2]), m, int(nn_best_params[4]), nn_best_params[5])\n",
    "\n",
    "    plot_loss(plot_df['loss_train'], plot_df['accuracy_train'], plot_df['loss_val'], plot_df['accuracy_val'],\n",
    "                  50, int(nn_best_params[1]), float(0.001), m, nn_best_params[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb242f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in layer_size:\n",
    "    plot_df, W1, b1, W2, b2 = gradient_descent_nn(X_train, Y_train, X_val, Y_val, int(nn_best_params[1]),\n",
    "                                float(nn_best_params[2]), float(nn_best_params[3]), layer, nn_best_params[5])\n",
    "\n",
    "    plot_loss(plot_df['loss_train'], plot_df['accuracy_train'], plot_df['loss_val'], plot_df['accuracy_val'],\n",
    "                  50, int(nn_best_params[1]), float(nn_best_params[2]), float(nn_best_params[3]), layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de26f93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for func in activation_functions:\n",
    "    plot_df, W1, b1, W2, b2 = gradient_descent_nn(X_train, Y_train, X_val, Y_val, int(nn_best_params[1]),\n",
    "                                float(nn_best_params[2]), float(nn_best_params[3]), int(nn_best_params[4]), func)\n",
    "\n",
    "    plot_loss(plot_df['loss_train'], plot_df['accuracy_train'], plot_df['loss_val'], plot_df['accuracy_val'],\n",
    "                  50, int(nn_best_params[1]), float(nn_best_params[2]), float(nn_best_params[3]), nn_best_params[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef3b973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

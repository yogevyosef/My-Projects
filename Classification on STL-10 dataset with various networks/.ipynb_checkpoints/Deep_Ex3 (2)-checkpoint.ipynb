{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d81f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moshe Hershkovitz 313123150\n",
    "#Zachary Berrih 328595194"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291725fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for inline plots in jupyter\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random  as rnd\n",
    "from sklearn.model_selection import train_test_split \n",
    "np.random.seed(42)  # fixing seed. Important for reproducibility!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e99ad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51fe709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_batch_size, test_batch_size, val_part, seed):\n",
    "    mean, std = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
    "    # transform data: scaling, augmentation, ...\n",
    "    train_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                          transforms.Normalize(mean, std),\n",
    "                                          transforms.RandomCrop((64,64)),\n",
    "                                          transforms.ColorJitter(brightness=4, saturation=2, hue=2),\n",
    "                                          transforms.RandomHorizontalFlip(p=0.5)])\n",
    "\n",
    "    test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean, std),\n",
    "                                         transforms.CenterCrop((64,64))])\n",
    "\n",
    "\n",
    "    # pytorch has a datasets class with predefined datasets that can be easily downloaded and manipulated\n",
    "    train = torchvision.datasets.STL10('./data', split='train', download=True,transform=train_transform)\n",
    "    val  = torchvision.datasets.STL10('./data', split='train', download=True,transform=test_transform)\n",
    "    test = torchvision.datasets.STL10('./data', split='test',download=True, transform=test_transform)\n",
    "\n",
    "    # subsample the training set to make it more interesting\n",
    "    #part_train = torch.utils.data.random_split(train, [nsamples, len(train)-nsamples])[0]\n",
    "\n",
    "    # Dataloader combines a dataset and a sampler, and provides an iterable over the given dataset\n",
    "    # Here I set num_workers to 1. Set it to 4 when working in computational rich environments.\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=train_batch_size, shuffle=True, num_workers=1)\n",
    "    val_loader  = torch.utils.data.DataLoader(val, batch_size=test_batch_size, shuffle=True, num_workers=1)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=test_batch_size, num_workers=1)\n",
    "\n",
    "    \n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d5c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf1c8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img,label):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    #plt.title('label {}'.format(label))\n",
    "    plt.title(classes[label])\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ae9870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(classes):\n",
    "    mean, std = (0.43, 0.42, 0.39), (0.27, 0.26, 0.27)\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean, std)])\n",
    "    trainset = torchvision.datasets.STL10(root='./data', split='train',\n",
    "                                         download=True, transform=vis_transform)\n",
    "  \n",
    "    n_samples = len(trainset)\n",
    "    plt.figure(figsize=(10,30))\n",
    "    for index, c in enumerate(classes):\n",
    "        counter = 0\n",
    "        while counter < 4:\n",
    "            random_index = int(np.random.random()*n_samples)\n",
    "            images, label = trainset[random_index]\n",
    "            if label == index:\n",
    "                plt.subplot(10,4, (counter+1) +label*4)\n",
    "                counter += 1\n",
    "                imshow(torchvision.utils.make_grid(images),label)            \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd096b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8761f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logistic regression over flattened version of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776fd697",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.flat = nn.Flatten(start_dim=1)\n",
    "        self.fc = nn.Linear(64 * 64 * 3, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flat(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eb1bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fully-connected NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1f370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN3Layers(nn.Module):\n",
    "    #64*64*3 = 12288\n",
    "    def __init__(self, image_size=12288):\n",
    "        super(NN3Layers, self).__init__()\n",
    "        # first layer\n",
    "        self.flat = nn.Flatten(start_dim=1)\n",
    "        self.fc1 = nn.Linear(image_size, 1500, bias=True)\n",
    "        self.BN1 = nn.BatchNorm1d(1500)\n",
    "        self.D1 = nn.Dropout(p=0.5)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        # second layer\n",
    "        self.fc2 = nn.Linear(1500, 500, bias=True)\n",
    "        self.BN2 = nn.BatchNorm1d(200)\n",
    "        self.Tanh2 = nn.Tanh()\n",
    "        self.D2 = nn.Dropout(p=0.5)\n",
    "        # third layer\n",
    "        self.fc3 = nn.Linear(500, 100, bias=True)\n",
    "        self.BN3 = nn.BatchNorm1d(100)\n",
    "        self.Tanh3 = nn.Tanh()\n",
    "        self.D3 = nn.Dropout(p=0.5)\n",
    "        self.fcend = nn.Linear(100, 10, bias=True) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flat(x)                    \n",
    "        # first layer\n",
    "        x = self.fc1(x)\n",
    "        x = self.BN1(x)\n",
    "        x = self.ReLU(x)\n",
    "        \n",
    "        # second layer\n",
    "        x = self.fc2(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.Tanh2(x)\n",
    "        x = self.D2(x)\n",
    "        \n",
    "        # third layer\n",
    "        x = self.fc3(x)\n",
    "        x = self.BN3(x)\n",
    "        x = self.Tanh3(x)\n",
    "        x = self.D3(x)\n",
    "                             \n",
    "        logits = self.fcend(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5df88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f0d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)#50*50*6\n",
    "        self.BN1 = nn.BatchNorm1d(6)\n",
    "        self.pool = nn.MaxPool2d(2, 2)#25*25\n",
    "        self.conv2 = nn.Conv2d(6, 10, 2)#24*24*10\n",
    "        self.BN2 = nn.BatchNorm1d(10)\n",
    "        self.conv3 = nn.Conv2d(10, 32, 3)#22*22*32\n",
    "        self.BN3 = nn.BatchNorm2d(32)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 100)\n",
    "        self.d1 = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        self.d2 = nn.Dropout(p=0.2)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.BN1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.BN2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.BN3(self.conv3(x))))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = self.d1(F.relu(self.fc1(x)))\n",
    "        x = self.d2(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81665ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__() #  64*64*3\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) # #50*50*6\n",
    "        self.bn1 = nn.BatchNorm2d(6)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2) #  25*25*6\n",
    "        self.conv2 = nn.Conv2d(6, 10, 2) #  24*24*10\n",
    "        self.bn2 = nn.BatchNorm2d(10)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2) #  12*12*10\n",
    "        self.conv3 = nn.Conv2d(10, 64, 1) #  12*12*64\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2) #  6*6*64\n",
    "        self.fc1 = nn.Linear(64 * 6 * 6, 100) #  100\n",
    "        self.d1 = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(100, 10) #  10\n",
    "        self.d2 = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 64 * 6 * 6) # matrix data to flat vector\n",
    "        x = self.d1(F.relu(self.fc1(x)))\n",
    "        x = self.d2(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ef9866",
   "metadata": {},
   "outputs": [],
   "source": [
    "###A fixed pre-trained MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53248121",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        self.feature_extractor = models.mobilenet.mobilenet_v2(pretrained=True)\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.feature_extractor.classifier[1] = nn.Linear(self.feature_extractor.classifier[1].in_features, out_features=200)\n",
    "       \n",
    "        self.d1 = nn.Dropout(p=0.2\n",
    "                            )\n",
    "        self.fc2 = nn.Linear(200, 20) \n",
    "        self.d2 = nn.Dropout(p=0.2)\n",
    "        self.fc3 = nn.Linear(20, 10) \n",
    "        self.d3 = nn.Dropout(p=0.2) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.d1(F.relu(x)) \n",
    "        x = self.d2(F.relu(self.fc2(x)))\n",
    "        output = self.d3(self.fc3(x))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b7539",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV2_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MobileNetV2_2, self).__init__()\n",
    "        self.feature_extractor = models.mobilenet.mobilenet_v2(pretrained=True) \n",
    "        self.feature_extractor.classifier[1] = nn.Linear(self.feature_extractor.classifier[1].in_features, out_features=200)\n",
    "       \n",
    "        self.d1 = nn.Dropout(p=0.2\n",
    "                            )\n",
    "        self.fc2 = nn.Linear(200, 20) \n",
    "        self.d2 = nn.Dropout(p=0.2)\n",
    "        self.fc3 = nn.Linear(20, 10) \n",
    "        self.d3 = nn.Dropout(p=0.2) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.d1(F.relu(x)) \n",
    "        x = self.d2(F.relu(self.fc2(x)))\n",
    "        output = self.d3(self.fc3(x))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b97f573",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f307d1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(train_loader, val_loader, model, device, hyper_parameters):\n",
    "    epochs, lr, momentum, weight_decay, optimizer_type = hyper_parameters\n",
    "    # loss function - cross entropy\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # optimizer type\n",
    "    if optimizer_type == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    elif optimizer_type == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=weight_decay)\n",
    "    elif optimizer_type == 'RMSProp':\n",
    "         optimizer = optim.RMSProp(model.parameters(), lr=lr, alpha=0.99, eps=1e-08, weight_decay=weight_decay)\n",
    "    else:\n",
    "         NotImplementedError(\"optimizer not implemented\")\n",
    "    train_losses, train_accuracy, val_losses, val_accuracy = ([] for i in range(4))\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        cumm_loss = 0\n",
    "        num_batch = 0\n",
    "        train_cor = 0.\n",
    "        train_tot = 0.\n",
    "\n",
    "        # iterate over the data\n",
    "        for (data, target) in train_loader:\n",
    "            num_batch += 1\n",
    "\n",
    "            # flatten the data tensor and move it to the GPU (when using a GPU)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Important! sets the gradients of all optimized torch.Tensors to zero. This is because by default,\n",
    "            # gradients are accumulated in buffers( i.e, not overwritten) whenever .backward() is called.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # pass data through the model\n",
    "            logits = model(data)\n",
    "\n",
    "            # suffer loss\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            # Use autograd to compute the backward pass. This call will compute the gradient of loss with respect\n",
    "            # to all Tensors with requires_grad=True. This operation will free the computation graph\n",
    "            loss.backward()\n",
    "\n",
    "            # Calling the step function on an Optimizer makes an update to its parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            cumm_loss += loss.item()\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            train_tot += target.size(0)\n",
    "            train_cor += (predicted == target).sum().item()\n",
    "\n",
    "        #claculate loss, and accuracy of the train and the validation, and print\n",
    "        train_losses.append(cumm_loss / num_batch)\n",
    "        train_accuracy.append(train_cor / train_tot)\n",
    "        curr_val_accuracy, curr_val_loss = test(val_loader, model, device)\n",
    "        val_accuracy.append(curr_val_accuracy)\n",
    "        val_losses.append(curr_val_loss)\n",
    "        print('epoch %d \\tloss: %.3f\\t  acc:%.3f\\t val_acc:%.3f' %\n",
    "            (epoch, cumm_loss / num_batch, train_cor / train_tot, curr_val_accuracy))\n",
    "    results = train_accuracy, train_losses, val_accuracy, val_losses\n",
    "\n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8792de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function for validation and test\n",
    "def test(test_loader, model, device):\n",
    "\n",
    "    # will notify all your layers that you are in eval mode, that way, \n",
    "    # batchnorm or dropout layers will work in eval mode instead of training mode\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    num_batch = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total = 0\n",
    "    \n",
    "    # torch.no_grad() impacts the autograd engine and deactivate it. \n",
    "    # It will reduce memory usage and speed up computations\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            num_batch += 1\n",
    "            images, labels= images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            # calculate the correct prediction\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    return correct / total, test_loss / num_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f589526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(results, epochs, lr, momentum, weight_decay, model_type):\n",
    "\n",
    "    train_accuracy, train_losses, val_accuracy, val_losses = results\n",
    "    steps = np.arange(epochs)\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.set_xlabel('epochs')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.plot(steps, train_losses, label=\"train loss\", color='red')\n",
    "    ax1.plot(steps, val_losses, label=\"val loss\", color='green')\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    ax2.set_ylabel('accuracy')  # we already handled the x-label with ax1\n",
    "    ax2.plot(steps, train_accuracy, label=\"train acc\", color='black')\n",
    "    ax2.plot(steps, val_accuracy, label=\"val acc\", color='blue')\n",
    "\n",
    "    plt.suptitle(model_type, fontsize = 16)\n",
    "    plt.title('epochs={}, learning rate={}, momentum={}, weight decay={}'.format(epochs, lr, momentum, weight_decay), fontsize = 10)\n",
    "\n",
    "    fig.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print (\" train accuracy is :\" + str(np.mean(train_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cda220",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "epochs = 100\n",
    "lr = 0.001 # 0.001 - LR. 0.1 - NN3, CNN, MobileNetV2.\n",
    "cuda = False\n",
    "seed = 42\n",
    "optimizer_type = 'SGD' # SGD, Adam, RMSprop\n",
    "momentum = 0.0\n",
    "weight_decay = 1e-4\n",
    "validation_part = 0.2\n",
    "classes = ('Airplane', 'Bird', 'Car', 'Cat', 'Deer', 'Dog', 'Horse', 'Monkey', 'Ship', 'Truck')\n",
    "\n",
    "use_cuda = cuda and torch.cuda.is_available()\n",
    "\n",
    "# Set seed \n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "# Handel GPU stochasticity\n",
    "torch.backends.cudnn.enabled = use_cuda\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "hyper_parameters = epochs, lr, momentum, weight_decay, optimizer_type\n",
    "\n",
    "train_loader, val_loader, test_loader = load_data(batch_size, test_batch_size, validation_part, seed)\n",
    "\n",
    "\n",
    "# model selection  \n",
    "model = LogisticRegression().to(device)\n",
    "#model = NN3Layers().to(device)\n",
    "#model = CNN().to(device)\n",
    "#model = MobileNetV2().to(device)  \n",
    "#model = MobileNetV2_2().to(device)\n",
    "\n",
    "# run the model\n",
    "model, results = run(train_loader, val_loader, model, device, hyper_parameters)\n",
    "test_acc, _ = test(test_loader, model, device)\n",
    "print(\"test acc:\", test_acc)\n",
    "plot_loss(results, epochs, lr, momentum, weight_decay, \"LogisticRegression\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fffdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "test_batch_size = 32\n",
    "epochs = 100\n",
    "lr = 0.01 \n",
    "cuda = False\n",
    "seed = 42\n",
    "optimizer_type = 'RMSprop' # SGD, Adam, RMSprop\n",
    "momentum = 0.01\n",
    "weight_decay = 1e-4\n",
    "validation_part = 0.2\n",
    "classes = ('Airplane', 'Bird', 'Car', 'Cat', 'Deer', 'Dog', 'Horse', 'Monkey', 'Ship', 'Truck')\n",
    "\n",
    "use_cuda = cuda and torch.cuda.is_available()\n",
    "\n",
    "# Set seed \n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "# Handel GPU stochasticity\n",
    "torch.backends.cudnn.enabled = use_cuda\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "hyper_parameters = epochs, lr, momentum, weight_decay, optimizer_type\n",
    "\n",
    "train_loader, val_loader, test_loader = load_data(batch_size, test_batch_size, validation_part, seed)\n",
    "\n",
    "\n",
    "# model selection  \n",
    "#model = LogisticRegression().to(device)\n",
    "model = NN3Layers().to(device)\n",
    "#model = CNN().to(device)\n",
    "#model = MobileNetV2().to(device)  \n",
    "#model = MobileNetV2_2().to(device)\n",
    "\n",
    "# run the model\n",
    "model, results = run(train_loader, val_loader, model, device, hyper_parameters)\n",
    "test_acc, _ = test(test_loader, model, device)\n",
    "print(\"test acc:\", test_acc)\n",
    "plot_loss(results, epochs, lr, momentum, weight_decay, \"NN3Layers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bff9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "epochs = 100\n",
    "lr = 0.1 # 0.001 - LR. 0.1 - NN3, CNN, MobileNetV2.\n",
    "cuda = False\n",
    "seed = 42\n",
    "optimizer_type = 'SGD' # SGD, Adam, RMSprop\n",
    "momentum = 0.0\n",
    "weight_decay = 1e-4\n",
    "validation_part = 0.2\n",
    "classes = ('Airplane', 'Bird', 'Car', 'Cat', 'Deer', 'Dog', 'Horse', 'Monkey', 'Ship', 'Truck')\n",
    "\n",
    "use_cuda = cuda and torch.cuda.is_available()\n",
    "\n",
    "# Set seed \n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "# Handel GPU stochasticity\n",
    "torch.backends.cudnn.enabled = use_cuda\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "hyper_parameters = epochs, lr, momentum, weight_decay, optimizer_type\n",
    "\n",
    "train_loader, val_loader, test_loader = load_data(batch_size, test_batch_size, validation_part, seed)\n",
    "\n",
    "\n",
    "# model selection  \n",
    "#model = LogisticRegression().to(device)\n",
    "#model = NN3Layers().to(device)\n",
    "model = CNN().to(device)\n",
    "#model = MobileNetV2().to(device)  \n",
    "#model = MobileNetV2_2().to(device)\n",
    "\n",
    "# run the model\n",
    "model, results = run(train_loader, val_loader, model, device, hyper_parameters)\n",
    "test_acc, _ = test(test_loader, model, device)\n",
    "print(\"test acc:\", test_acc)\n",
    "plot_loss(results, epochs, lr, momentum, weight_decay, \"CNN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07574241",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "epochs = 100\n",
    "lr = 0.1 # 0.001 - LR. 0.1 - NN3, CNN, MobileNetV2.\n",
    "cuda = False\n",
    "seed = 42\n",
    "optimizer_type = 'SGD' # SGD, Adam, RMSprop\n",
    "momentum = 0.0\n",
    "weight_decay = 1e-4\n",
    "validation_part = 0.2\n",
    "classes = ('Airplane', 'Bird', 'Car', 'Cat', 'Deer', 'Dog', 'Horse', 'Monkey', 'Ship', 'Truck')\n",
    "\n",
    "use_cuda = cuda and torch.cuda.is_available()\n",
    "\n",
    "# Set seed \n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "# Handel GPU stochasticity\n",
    "torch.backends.cudnn.enabled = use_cuda\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "hyper_parameters = epochs, lr, momentum, weight_decay, optimizer_type\n",
    "\n",
    "train_loader, val_loader, test_loader = load_data(batch_size, test_batch_size, validation_part, seed)\n",
    "\n",
    "\n",
    "# model selection  \n",
    "#model = LogisticRegression().to(device)\n",
    "#model = NN3Layers().to(device)\n",
    "#model = CNN().to(device)\n",
    "model = MobileNetV2().to(device)  \n",
    "#model = MobileNetV2_2().to(device)\n",
    "\n",
    "# run the model\n",
    "model, results = run(train_loader, val_loader, model, device, hyper_parameters)\n",
    "test_acc, _ = test(test_loader, model, device)\n",
    "print(\"test acc:\", test_acc)\n",
    "plot_loss(results, epochs, lr, momentum, weight_decay, \"MobileNetV2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21213ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "epochs = 100\n",
    "lr = 0.1 # 0.001 - LR. 0.1 - NN3, CNN, MobileNetV2.\n",
    "cuda = False\n",
    "seed = 42\n",
    "optimizer_type = 'SGD' # SGD, Adam, RMSprop\n",
    "momentum = 0.0\n",
    "weight_decay = 1e-4\n",
    "validation_part = 0.2\n",
    "classes = ('Airplane', 'Bird', 'Car', 'Cat', 'Deer', 'Dog', 'Horse', 'Monkey', 'Ship', 'Truck')\n",
    "\n",
    "use_cuda = cuda and torch.cuda.is_available()\n",
    "\n",
    "# Set seed \n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "# Handel GPU stochasticity\n",
    "torch.backends.cudnn.enabled = use_cuda\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "hyper_parameters = epochs, lr, momentum, weight_decay, optimizer_type\n",
    "\n",
    "train_loader, val_loader, test_loader = load_data(batch_size, test_batch_size, validation_part, seed)\n",
    "\n",
    "\n",
    "# model selection  \n",
    "#model = LogisticRegression().to(device)\n",
    "#model = NN3Layers().to(device)\n",
    "#model = CNN().to(device)\n",
    "#model = MobileNetV2().to(device)  \n",
    "model = MobileNetV2_2().to(device)\n",
    "\n",
    "# run the model\n",
    "model, results = run(train_loader, val_loader, model, device, hyper_parameters)\n",
    "test_acc, _ = test(test_loader, model, device)\n",
    "print(\"test acc:\", test_acc)\n",
    "plot_loss(results, epochs, lr, momentum, weight_decay, \"MobileNetV2_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2abe609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
